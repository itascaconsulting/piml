import numpy as np
import matplotlib; matplotlib.rcParams["savefig.directory"] = "."
from matplotlib import pyplot as plt
plt.rcParams.update({'font.size': 18})
import sklearn
from sklearn.neural_network import MLPRegressor

weights = []
bias = []
for i in range(7):
    weights.append(np.load(f"weights_{i}.npy"))
    bias.append(np.load(f"bias_{i}.npy"))

#sc = joblib.load("sc.pkl") # python version conflict?
#ysc = joblib.load("ysc.pkl")


sc =  {'with_mean': True,
       'with_std': True,
       'copy': True,
       'n_features_in_': 4,
       'n_samples_seen_': 209713,
       'mean_': np.array([2.54974369e+09, 2.55046925e+09, 2.55177105e+09, 2.55000189e+09]),
       'var_': np.array([2.00020019e+18, 2.00119520e+18, 2.00025171e+18, 2.00091493e+18]),
       'scale_': np.array([1.41428434e+09, 1.41463606e+09, 1.41430255e+09, 1.41453700e+09])}

ysc = {'with_mean': True,
       'with_std': True,
       'copy': True,
       'n_features_in_': 110,
       'n_samples_seen_': 209713,
       'mean_': np.array([ 0.00000000e+00,  0.00000000e+00, -9.11827558e-05,  0.00000000e+00,
                        0.00000000e+00,  2.84070729e-04, -8.73853497e-05,  2.52352583e-04,
                        -1.52415853e-04,  0.00000000e+00, -1.41999967e-04,  1.80051410e-04,
                        -1.73866681e-04,  0.00000000e+00, -1.57014200e-04,  1.06294471e-04,
                        -1.64480476e-04,  0.00000000e+00, -1.45187863e-04,  5.13775899e-05,
                        -1.38710090e-04,  0.00000000e+00, -1.21047230e-04,  1.71296231e-05,
                        -1.07621362e-04,  0.00000000e+00, -9.36635338e-05, -1.89230903e-06,
                        -7.70845928e-05,  0.00000000e+00, -6.72264091e-05, -1.14719362e-05,
                        -4.91775044e-05,  0.00000000e+00, -4.30489386e-05, -1.57983523e-05,
                        -2.38451050e-05,  0.00000000e+00, -2.09396808e-05, -1.74797373e-05,
                        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -1.78973989e-05,
                        0.00000000e+00,  6.12102013e-04, -9.30896286e-05,  5.28593222e-04,
                        -1.35509315e-04,  3.52922435e-04, -1.32442613e-04,  1.94224824e-04,
                        -1.12057843e-04,  8.74752468e-05, -8.89769248e-05,  2.53023199e-05,
                        -6.73008938e-05, -7.45144907e-06, -4.79387567e-05, -2.28911517e-05,
                        -3.06925738e-05, -2.91848188e-05, -1.49593716e-05, -3.12637845e-05,
                        0.00000000e+00, -3.16939784e-05,  0.00000000e+00,  1.04464849e-03,
                        -7.91708497e-05,  8.45846576e-04, -8.05013289e-05,  4.89822390e-04,
                        -5.26084483e-05,  2.41249264e-04, -3.78510679e-05,  9.93744456e-05,
                        -3.06717825e-05,  2.30989128e-05, -2.59117382e-05, -1.44528875e-05,
                        -2.09408865e-05, -3.08463376e-05, -1.48711614e-05, -3.67793913e-05,
                        -7.73909483e-06, -3.83316000e-05,  0.00000000e+00, -3.85439689e-05,
                        0.00000000e+00,  1.68883132e-03,  1.48387232e-04,  1.18088519e-03,
                        1.77729630e-04,  5.09276628e-04,  1.31537936e-04,  2.51794820e-04,
                        7.93853820e-05,  1.06002394e-04,  3.90740698e-05,  2.69814307e-05,
                        1.31164613e-05, -1.28978083e-05, -5.06714867e-07, -3.10554500e-05,
                        -5.08142429e-06, -3.81781142e-05, -3.91562964e-06, -4.04031393e-05,
                        0.00000000e+00, -4.08318056e-05]),
       'var_': np.array([0.00000000e+00, 0.00000000e+00, 4.39769521e-09, 0.00000000e+00,
                      0.00000000e+00, 1.03457269e-07, 3.18678219e-09, 8.86706657e-08,
                      1.28650816e-08, 0.00000000e+00, 8.85392693e-09, 5.74286643e-08,
                      1.79261911e-08, 0.00000000e+00, 1.16679712e-08, 2.96199216e-08,
                      1.73841313e-08, 0.00000000e+00, 1.08443393e-08, 1.24778523e-08,
                      1.34090402e-08, 0.00000000e+00, 8.14388734e-09, 4.16605893e-09,
                      8.68473878e-09, 0.00000000e+00, 5.19336591e-09, 9.90045776e-10,
                      4.72995003e-09, 0.00000000e+00, 2.80324917e-09, 2.21608414e-10,
                      2.01135390e-09, 0.00000000e+00, 1.18596495e-09, 3.33674338e-10,
                      4.85681132e-10, 0.00000000e+00, 2.85612068e-10, 5.96743547e-10,
                      0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.10687250e-10,
                      0.00000000e+00, 2.46820657e-07, 4.43864492e-09, 2.03510993e-07,
                      9.57804822e-09, 1.21404970e-07, 9.17206182e-09, 5.76928900e-08,
                      6.30425473e-09, 2.24036362e-08, 3.69341889e-09, 6.76169291e-09,
                      1.95799030e-09, 1.40787975e-09, 9.35990463e-10, 4.14920050e-10,
                      3.69755648e-10, 8.10250116e-10, 8.62781752e-11, 1.33875137e-09,
                      0.00000000e+00, 1.55029112e-09, 0.00000000e+00, 5.34797189e-07,
                      8.47050767e-09, 3.85265264e-07, 1.11226025e-08, 1.81059523e-07,
                      5.89583030e-09, 7.32837622e-08, 2.38951907e-09, 2.58423212e-08,
                      9.24625140e-10, 7.47703533e-09, 4.13058578e-10, 1.67438202e-09,
                      2.12145243e-10, 6.50568480e-10, 9.78656767e-11, 1.06910860e-09,
                      2.57256859e-11, 1.62064406e-09, 0.00000000e+00, 1.84159979e-09,
                      0.00000000e+00, 1.61032504e-06, 8.12827167e-09, 6.60284684e-07,
                      1.29005556e-08, 1.82056674e-07, 1.06977195e-08, 7.43118556e-08,
                      7.70808377e-09, 2.67482950e-08, 4.90536317e-09, 8.00468240e-09,
                      2.77159897e-09, 1.87270691e-09, 1.37537211e-09, 6.66273055e-10,
                      5.53701875e-10, 1.01434135e-09, 1.30343693e-10, 1.54931284e-09,
                      0.00000000e+00, 1.76893271e-09]),
       'scale_': np.array([1.00000000e+00, 1.00000000e+00, 6.63151205e-05, 1.00000000e+00,
                        1.00000000e+00, 3.21647741e-04, 5.64515915e-05, 2.97776201e-04,
                        1.13424343e-04, 1.00000000e+00, 9.40953077e-05, 2.39642785e-04,
                        1.33888726e-04, 1.00000000e+00, 1.08018384e-04, 1.72104391e-04,
                        1.31848896e-04, 1.00000000e+00, 1.04136158e-04, 1.11704307e-04,
                        1.15797410e-04, 1.00000000e+00, 9.02434892e-05, 6.45450148e-05,
                        9.31919459e-05, 1.00000000e+00, 7.20650117e-05, 3.14649929e-05,
                        6.87746322e-05, 1.00000000e+00, 5.29457191e-05, 1.48865179e-05,
                        4.48481203e-05, 1.00000000e+00, 3.44378418e-05, 1.82667550e-05,
                        2.20381744e-05, 1.00000000e+00, 1.69000612e-05, 2.44283349e-05,
                        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 2.66587181e-05,
                        1.00000000e+00, 4.96810484e-04, 6.66231561e-05, 4.51121927e-04,
                        9.78675034e-05, 3.48432160e-04, 9.57708819e-05, 2.40193443e-04,
                        7.93993371e-05, 1.49678443e-04, 6.07735048e-05, 8.22295136e-05,
                        4.42491841e-05, 3.75217237e-05, 3.05939612e-05, 2.03695864e-05,
                        1.92290314e-05, 2.84648927e-05, 9.28860459e-06, 3.65889514e-05,
                        1.00000000e+00, 3.93737364e-05, 1.00000000e+00, 7.31298290e-04,
                        9.20353610e-05, 6.20697401e-04, 1.05463750e-04, 4.25510896e-04,
                        7.67843103e-05, 2.70709738e-04, 4.88827073e-05, 1.60755470e-04,
                        3.04076494e-05, 8.64698522e-05, 2.03238426e-05, 4.09192133e-05,
                        1.45652066e-05, 2.55062439e-05, 9.89270826e-06, 3.26972261e-05,
                        5.07204947e-06, 4.02572237e-05, 1.00000000e+00, 4.29138648e-05,
                        1.00000000e+00, 1.26898583e-03, 9.01569280e-05, 8.12579032e-04,
                        1.13580613e-04, 4.26680998e-04, 1.03429780e-04, 2.72602010e-04,
                        8.77956934e-05, 1.63549060e-04, 7.00382979e-05, 8.94688907e-05,
                        5.26459777e-05, 4.32747837e-05, 3.70860097e-05, 2.58122656e-05,
                        2.35308707e-05, 3.18487260e-05, 1.14168162e-05, 3.93613114e-05,
                        1.00000000e+00, 4.20586817e-05])}
mlpr = MLPRegressor(
    hidden_layer_sizes=(1000,1000,800,700,600,500),
    activation='relu',
    solver='sgd',
    learning_rate='adaptive',
    learning_rate_init=0.01,
    momentum=0.90,
    max_iter= 1,
    tol=2.5e-5,
    n_iter_no_change=1,
    random_state=1,
    shuffle=True,
    verbose=True
)


X = np.load("cube_4_13.npy")
Y = np.load("result_cube_4_13.npy")
scaler = sklearn.preprocessing.StandardScaler()
yscaler = sklearn.preprocessing.StandardScaler()

for k,v in sc.items():
    scaler.__dict__[k] = v

for k,v in ysc.items():
    yscaler.__dict__[k] = v

xs = scaler.transform(X)
ys = yscaler.transform(Y.reshape(len(Y),110))

mlpr.fit(xs,ys)
# let this go for a while then, C-c to stop it

mlpr.coefs_ = weights
mlpr.intercepts_ = bias

xt = np.load("tc_X.npy")
yat = np.load("tc_Ya.npy")
ypt = np.load("tc_Yp.npy")

p = yscaler.inverse_transform(mlpr.predict(scaler.transform([xt])))
print(p)
print(ypt)
